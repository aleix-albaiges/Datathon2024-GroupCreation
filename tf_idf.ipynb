{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "from rich import print\n",
    "import heapq\n",
    "import uuid\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "from elasticsearch_dsl import Search, Index, analyzer, tokenizer\n",
    "from elasticsearch_dsl.query import Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There are <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">924</span> documents in the index <span style=\"color: #008000; text-decoration-color: #008000\">'technical_ind'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "There are \u001b[1;36m924\u001b[0m documents in the index \u001b[32m'technical_ind'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 924/924 [00:11<00:00, 83.65it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There are <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">924</span> documents in the index <span style=\"color: #008000; text-decoration-color: #008000\">'objective_ind'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "There are \u001b[1;36m924\u001b[0m documents in the index \u001b[32m'objective_ind'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 924/924 [00:08<00:00, 106.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "index_names = ['technical_ind', 'objective_ind']\n",
    "corpuses = {'technical_ind':{}, 'objective_ind':{}}\n",
    "for index_name in index_names:\n",
    "    ndocs = int(client.cat.count(index=index_name, format = \"json\")[0]['count'])\n",
    "    print(f\"There are {ndocs} documents in the index '{index_name}'\")\n",
    "\n",
    "\n",
    "    corpus = corpuses[index_name]    # will store _normalized_ tfidf for each document, key is internal elasticsearch id, value is dictionary of term -> tf-idf weight\n",
    "    for s in tqdm.tqdm(scan(client, index=index_name, query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "        terms = []\n",
    "        freqs = []\n",
    "        dfs = []\n",
    "\n",
    "        tv = client.termvectors(index=index_name, id=s['_id'], fields=['text'], term_statistics=True, positions=False)\n",
    "        if 'text' in tv['term_vectors']:   # just in case some document has no field named 'text'\n",
    "            for t in tv['term_vectors']['text']['terms']:\n",
    "                f = tv['term_vectors']['text']['terms'][t]['term_freq']\n",
    "\n",
    "                terms.append(t)\n",
    "                freqs.append(tv['term_vectors']['text']['terms'][t]['term_freq'])\n",
    "                dfs.append(tv['term_vectors']['text']['terms'][t]['doc_freq'])\n",
    "\n",
    "        # vector computations for tf-idf; l2-normalized for further calculations..\n",
    "        tfidf = np.array(freqs) * np.log2(ndocs / np.array(dfs))\n",
    "        tfidf /= np.linalg.norm(tfidf)\n",
    "\n",
    "        # save in corpus dictionary\n",
    "        corpus[s['_source']['path']] = {t: tfidf[j] for j, t in enumerate(terms)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(d: list[tuple[str, float]]) -> float:\n",
    "    return np.sqrt(sum([freq*freq for _, freq in d]))\n",
    "\n",
    "\n",
    "def normalize(d1: list[tuple[str, float]]):\n",
    "    normm = norm(d1)\n",
    "    return [(k, v/normm) for k, v in d1]\n",
    "\n",
    "def stemmer(query: str) -> str:\n",
    "    res = ind.analyze(body={'analyzer':'default', 'text': query})\n",
    "    query_stemmed = ''\n",
    "    first = True\n",
    "    for r in res['tokens']:\n",
    "        if not first:\n",
    "            query_stemmed += ' ' + r['token']\n",
    "        else:\n",
    "            query_stemmed += r['token']\n",
    "            first = False\n",
    "    return query_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective_ind similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 924/924 [00:00<00:00, 6587.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "r = 10  # only return r top docs\n",
    "queries = ['win prize many top dive trophy limit victory','learn skills dive improve gain experience', 'first try begin people knowledge start','level experiment journey collaborate experience']\n",
    "sims_ob : dict[str, dict[int,float]] = {}\n",
    "\n",
    "l2query  = [np.sqrt(len(query.split())) for query in queries]  # l2 of query assuming 0-1 vector representation\n",
    "\n",
    "# get nr. of docs; just for the progress bar\n",
    "ndocs = int(client.cat.count(index='objective_ind', format = \"json\")[0]['count'])\n",
    "\n",
    "# scan through docs, compute cosine sim between query and each doc\n",
    "for s in tqdm.tqdm(scan(client, index='objective_ind', query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "    \n",
    "    docid = s['_source']['path']   # use path as id\n",
    "    weights = corpuses['objective_ind'][docid]   # gets weights as a python dict of term -> weight (see remark above)\n",
    "    docid = docid.split('/')[-1].replace('.txt', '')\n",
    "    sims_ob[docid] = {}\n",
    "    for i in range(len(queries)):\n",
    "        sims_ob[docid][i] = 0.0\n",
    "        for w in queries[i].split():  # gets terms as a list\n",
    "            if w in weights:    # probably need to do something fancier to make sure that word is in vocabulary etc.\n",
    "                sims_ob[docid][i] += weights[w]   # accumulates if w in current doc\n",
    "        # normalize sim\n",
    "        sims_ob[docid][i] /= l2query[i]\n",
    "\n",
    "# now sort by cosine similarity\n",
    "#sorted_answer = sorted(sims.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "#pprint(sorted_answer[:r])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print de les sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print('OBJECTIVE INDEX')\n",
    "for i in sims_ob.keys():\n",
    "    if sims_ob[i][2] > 0.1:\n",
    "        print(i, sims_ob[i])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical_ind similarities (no funciona, no sé per què)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 924/924 [00:00<00:00, 3787.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "r = 10  # only return r top docs\n",
    "query = 'python react postgreSQL figma c++ java react pytorch sql html/css mongodb google flutter amazon raspberry tensorflow ar/vr'\n",
    "sims_technical : dict[str, float] = {}\n",
    "\n",
    "l2query  = np.sqrt(len(query.split()))  # l2 of query assuming 0-1 vector representation\n",
    "\n",
    "# get nr. of docs; just for the progress bar\n",
    "ndocs = int(client.cat.count(index='technical_ind', format = \"json\")[0]['count'])\n",
    "\n",
    "# scan through docs, compute cosine sim between query and each doc\n",
    "for s in tqdm.tqdm(scan(client, index='technical_ind', query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "    docid = s['_source']['path']   # use path as id\n",
    "    weights = corpuses['technical_ind'][docid]   # gets weights as a python dict of term -> weight (see remark above)\n",
    "    docid = docid.split('/')[-1].replace('.txt', '')\n",
    "    sims_technical[docid] = 0.0\n",
    "\n",
    "    for w in query.split():  # gets terms as a list\n",
    "        if w in weights:    # probably need to do something fancier to make sure that word is in vocabulary etc.\n",
    "                \n",
    "            sims_technical[docid] += weights[w]   # accumulates if w in current doc\n",
    "        # normalize sim\n",
    "    sims_technical[docid] /= l2query\n",
    "\n",
    "#sims_technical = {key: sum(subdict.values()) for key, subdict in sims_technical.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00\">329773c9-9ea9-45c8-84bb-d68427073632</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10198868761551974</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[93m329773c9-9ea9-45c8-84bb-d68427073632\u001b[0m \u001b[1;36m0.10198868761551974\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00\">8a354d53-8533-4a75-96d9-5fb93ce2019d</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1260346529366269</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[93m8a354d53-8533-4a75-96d9-5fb93ce2019d\u001b[0m \u001b[1;36m0.1260346529366269\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00\">b1948e98-e330-4d55-aaeb-a0672ccfadc4</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.11716980838428814</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[93mb1948e98-e330-4d55-aaeb-a0672ccfadc4\u001b[0m \u001b[1;36m0.11716980838428814\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00\">c8536faf-fdc9-4617-8088-a2ec9a08c8b4</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1004167471155284</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[93mc8536faf-fdc9-4617-8088-a2ec9a08c8b4\u001b[0m \u001b[1;36m0.1004167471155284\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key, value in sims_technical.items():\n",
    "    if value > 0.1:\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print('OBJECTIVE INDEX')\n",
    "for i in sims_technical.keys():\n",
    "    if sims_technical[i] > 0.01:\n",
    "        print(i, sims_technical[i])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROVES DIVERSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from participant import load_participants\n",
    "from rich import print\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "data_path = \"data/datathon_participants.json\"\n",
    "participants = load_participants(data_path)\n",
    "\n",
    "objectives : dict[uuid.UUID,str] = {}\n",
    "technical : dict[uuid.UUID,str] = {}\n",
    "\n",
    "\n",
    "for p in participants:\n",
    "    objectives[p.id] = p.objective + \" \" + p.introduction\n",
    "    technical[p.id] = p.technical_project + \" \" + p.future_excitement\n",
    "\n",
    "\n",
    "word_counts : dict[str,int] = {}\n",
    "for key, value in technical.items():\n",
    "    for word in value.split():\n",
    "        if word not in word_counts: \n",
    "            word_counts[word] = 0\n",
    "        word_counts[word] += 1\n",
    "\n",
    "sorted_answer = sorted(word_counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(sorted_answer)\n",
    "print(objectives['fcee953a-30c6-475a-b65c-ec49223281e9'])\n",
    "\n",
    "text = 'Objectives_files/fcee953a-30c6-475a-b65c-ec49223281e9.txt'\n",
    "resultat = text.split('/')[-1].replace('.txt', '')\n",
    "print(resultat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> matches.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Found \u001b[1;36m10\u001b[0m matches.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Hit</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">technical_ind/MN0GNZMBzkPY1-yrjw9W</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'path'</span><span style=\"color: #000000; text-decoration-color: #000000\">: 'Technical_files/329773c9-9ea9-45c8-84bb-d684270736</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;35mHit\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mtechnical_ind/MN0GNZMBzkPY1-yrjw9W\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'path'\u001b[0m\u001b[39m: 'Technical_files/329773c9-9ea9-45c8-84bb-d684270736\u001b[0m\u001b[33m...\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from elasticsearch_dsl import Search\n",
    "\n",
    "# the following search query specifies the field where we want to search\n",
    "s_obj = Search(using=client, index='technical_ind')\n",
    "sq = s_obj.query('match', text='sql')\n",
    "resp = sq.execute()\n",
    "\n",
    "print(f'Found {len(resp)} matches.')\n",
    "\n",
    "for hit in resp:\n",
    "    print(hit)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Literal\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Participant:\n",
    "    id: uuid.UUID  # Unique identifier\n",
    "\n",
    "    # Personal data\n",
    "    name: str\n",
    "    email: str\n",
    "    age: int\n",
    "    year_of_study: Literal[\"1st year\", \"2nd year\", \"3rd year\", \"4th year\", \"Masters\", \"PhD\"]\n",
    "    shirt_size: Literal[\"S\", \"M\", \"L\", \"XL\"]\n",
    "    university: str\n",
    "    dietary_restrictions: Literal[\"None\", \"Vegetarian\", \"Vegan\", \"Gluten-free\", \"Other\"]\n",
    "\n",
    "    # Experience and programming skills\n",
    "    programming_skills: Dict[str, int]\n",
    "    experience_level: Literal[\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
    "    hackathons_done: int\n",
    "\n",
    "    # Interests, preferences and constraints\n",
    "    interests: List[str]\n",
    "    preferred_role: Literal[\n",
    "        \"Analysis\", \"Visualization\", \"Development\", \"Design\", \"Don't know\", \"Don't care\"\n",
    "    ]\n",
    "\n",
    "    interest_in_challenges: List[str]\n",
    "    preferred_languages: List[str]\n",
    "    friend_registration: List[uuid.UUID]\n",
    "    preferred_team_size: int\n",
    "    availability: Dict[str, bool]\n",
    "\n",
    "    # Description of the participant\n",
    "    Tryhard: float = 0\n",
    "    Learner: float = 0\n",
    "    Rookie: float = 0\n",
    "    Portfolio: float = 0\n",
    "    Experience : float = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_participants(path: str, sims: dict[uuid.UUID, list[float]]) -> list[Participant]:\n",
    "    if not pathlib.Path(path).exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"The file {path} does not exist, are you sure you're using the correct path?\"\n",
    "        )\n",
    "    if not pathlib.Path(path).suffix == \".json\":\n",
    "        raise ValueError(\n",
    "            f\"The file {path} is not a JSON file, are you sure you're using the correct file?\"\n",
    "        )\n",
    "\n",
    "    participants_data = json.load(open(path))\n",
    "    participants = []\n",
    "    \n",
    "    for participant_data in participants_data:\n",
    "        # Remove fields that aren't in the Participant class\n",
    "        if 'objective' in participant_data:\n",
    "            del participant_data['objective']\n",
    "        if 'introduction' in participant_data:\n",
    "            del participant_data['introduction']\n",
    "        if 'fun_fact' in participant_data:\n",
    "            del participant_data['fun_fact']\n",
    "        if 'future_excitement' in participant_data:\n",
    "            del participant_data['future_excitement']\n",
    "        if 'technical_project' in participant_data:\n",
    "            del participant_data['technical_project']\n",
    "            \n",
    "        # Convert the ID string to UUID\n",
    "        participant_id = str(participant_data['id'])\n",
    "        \n",
    "        # If this participant has simulation data, update their values\n",
    "        if participant_id in sims.keys():\n",
    "            participant_data['Tryhard'] = sims[participant_id][0]\n",
    "            participant_data['Learner'] = sims[participant_id][1]\n",
    "            participant_data['Rookie'] = sims[participant_id][2]\n",
    "            participant_data['Portfolio'] = sims[participant_id][3]\n",
    "            participant_data['Experience'] = sims_technical[participant_id]\n",
    "          \n",
    "        # Create the participant instance with the updated data\n",
    "        participants.append(Participant(**participant_data))\n",
    "    \n",
    "    return participants\n",
    "\n",
    "\n",
    "participants1 = load_participants('data/datathon_participants.json', sims_ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def participants_to_csv(participants: List[Participant], output_file: str):\n",
    "    \"\"\"Convert list of Participant objects to CSV file.\"\"\"\n",
    "    # Get all fields from the first participant\n",
    "    fieldnames = [field for field in vars(participants[0])]\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write header\n",
    "        writer.writerow(fieldnames)\n",
    "        \n",
    "        # Write each participant's data\n",
    "        for participant in participants:\n",
    "            row = []\n",
    "            for field in fieldnames:\n",
    "                value = getattr(participant, field)\n",
    "                # Convert complex types to strings\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    value = str(value)\n",
    "                row.append(value)\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Example usage\n",
    "participants_to_csv(participants1, 'participants.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
